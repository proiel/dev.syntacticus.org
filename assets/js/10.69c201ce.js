(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{282:function(e,t,o){"use strict";o.r(t);var r=o(14),a=Object(r.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("If you are new to Syntacticus, and would like to contribute code, we ask that you first consider working on one of our Beginners' exercises, so to get a feel for what Syntacticus is.")]),e._v(" "),t("h2",{attrs:{id:"ideas-for-gsoc-projects"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ideas-for-gsoc-projects"}},[e._v("#")]),e._v(" Ideas for GSoC projects")]),e._v(" "),t("h3",{attrs:{id:"front-end-projects"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#front-end-projects"}},[e._v("#")]),e._v(" Front-end projects")]),e._v(" "),t("ul",[t("li",[t("p",[t("em",[e._v("Improve visualisation of dependency structures")]),e._v(". Visualising dependency structures is a key part of what the "),t("a",{attrs:{href:"http://syntacticus.org",target:"_blank",rel:"noopener noreferrer"}},[e._v("Syntacticus"),t("OutboundLink")],1),e._v(" front-end is supposed to do, but it is a surprisingly hard thing to get right, especially when the structures grow and become complicated. There are a few common layouts that people are used to seeing, but they do not necessarily scale that well. We've historically used graphviz to generate graphs that are readable, but running this in the browser is a bit too resource intensive. We've instead tried to come up with different ways of visualising the dependencies using "),t("a",{attrs:{href:"https://d3js.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("D3.js"),t("OutboundLink")],1),e._v(", but we'd love it if you could improve on them! "),t("em",[e._v("You need: Javascript, D3.js, Vue.js")])])]),e._v(" "),t("li",[t("p",[t("em",[e._v("Intelligent full-text search")]),e._v(". Our search facilities are pretty basic at the moment. The long-term idea is to adopt an approach that allows us to present users with a search-engine type UI: one search box, using which the user can search all the resources in our data set at once. We need to search and rank matches across the texts themselves, the annotation, dictionaries and metadata. This is in part an exercise in UI design and UX, and in part about experimenting with ways of ranking search results. You will have to work with both the front end and the back-end API on this project. "),t("em",[e._v("You need: Javascript, Ruby")])])]),e._v(" "),t("li",[t("p",[t("em",[e._v("Implement smart cross-references to external resources")]),e._v(". There are plenty of resources like dictionaries and text collections out there that contain information that should be cross-referenced with Syntacticus. For some resources we already have IDs in out data set that would allow us to hook the resources together -- someone just has to figure out how to implement it and make it work. For others, you'd have to do some data matching yourself. Here are a couple of obvious versions of this project:")]),e._v(" "),t("ul",[t("li",[t("p",[t("em",[e._v("Link our Latin and Ancient Greek resources to the Persesus project")]),e._v(". The Perseus project have a wealth of resources that are relevant: The Lewis and Short, and the Lidell-Scott-Jones dictionaries, the "),t("a",{attrs:{href:"http://catalog.perseus.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Perseus Catalog"),t("OutboundLink")],1),e._v(" and translations of the texts that are of great utility to our users. We don't have information in our data set that allows us to make one-to-one references with head words in dictionaries or paragraphs in texts, but we can use heuristics to try to match them. To correct errors we'd need a feedback mechanism that allows our users to correct incorrect references. "),t("em",[e._v("You need: Javascript, Ruby")])])]),e._v(" "),t("li",[t("p",[t("em",[e._v("Link our Gothic text to the Wulfila project")]),e._v(". Our "),t("a",{attrs:{href:"http://syntacticus.org/source/proiel:20170214:gothic-nt",target:"_blank",rel:"noopener noreferrer"}},[e._v("Gothic New Testament text"),t("OutboundLink")],1),e._v(" comes from the brilliant "),t("a",{attrs:{href:"http://www.wulfila.be/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wulfila project"),t("OutboundLink")],1),e._v(". We inherited the text itself, lemmatisation and morphological annotation from this project, and we've preserved references to their digitised version of "),t("a",{attrs:{href:"http://www.wulfila.be/lib/streitberg/1910/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wilhelm Streitberg's Gotisch-Griechisch-Deutsches WÃ¶rterbuch"),t("OutboundLink")],1),e._v(". Exposing that information on "),t("a",{attrs:{href:"http://syntacticus.org",target:"_blank",rel:"noopener noreferrer"}},[e._v("Syntacticus"),t("OutboundLink")],1),e._v(" would be really cool! All the identifiers that you need are in the treebank XML files, but the backend API does not yet expose them. A challenge is that some of the annotation has been altered and now differs from Wulfila's annotation. You will have to figure out what has changed and how to deal with that. "),t("em",[e._v("You need: Javascript, D3.js, Ruby")])])])])])]),e._v(" "),t("h3",{attrs:{id:"toolkit-projects"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#toolkit-projects"}},[e._v("#")]),e._v(" Toolkit projects")]),e._v(" "),t("ul",[t("li",[t("p",[t("em",[e._v("Write a CorpusReader for NLTK")]),e._v(". PROIEL XML contains a lot of information beyond just text, and getting this easily into NLTK would be a huge win for. You'd have to be quite comfortable working with Python and XML to complete this project, and you should know the basics of NLTK. "),t("em",[e._v("You need: Python.")]),e._v(" Tip: read the "),t("a",{attrs:{href:"http://www.nltk.org/book/",target:"_blank",rel:"noopener noreferrer"}},[e._v("The NLTK book"),t("OutboundLink")],1),e._v(" before applying.")])]),e._v(" "),t("li",[t("p",[t("em",[e._v("Add pre-annotation support for unannotated PROIEL XML")]),e._v(". Our "),t("a",{attrs:{href:"https://github.com/mlj/proiel-webapp",target:"_blank",rel:"noopener noreferrer"}},[e._v("annotation tool"),t("OutboundLink")],1),e._v(" does not support using statistical taggers or parsers during online annotation. This is bad because it slows down annotators and increases the error rate. Improving this situation is a big task, because our tool is a complex applicatation and in need of serious a redesign, but we can get much of the benefit with less work by pre-annotating texts using taggers/parsers trained on the same language. Our command-line toolchain can be extended to incorporate this functionality. It relies on processing PROIEL XML files, so you'd have to design a module that accepts PROIEL XML without any annotation, then iterates sentences and/or tokens and invokes the external tagger. You'll have to consider which taggers/parsers are most suitable, and think about how to deal with tokenisation of sentences. The "),t("a",{attrs:{href:"http://www.jlcl.org/2011_Heft2/7.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("two"),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"http://www.aclweb.org/anthology/W/W16/W16-4009.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("papers"),t("OutboundLink")],1),e._v(" will give you some background to the problem. Be realistic when putting together a project on the basis of this idea; morphological tagging is probably more realistic than syntactic parsing. It is important to remember that this must be robust enough and reliable enough for use several years into the future. "),t("em",[e._v("You need: Ruby, a bit of experience with taggers")])])]),e._v(" "),t("li",[t("p",[t("em",[e._v("Automatically train POS-taggers and/or morphological taggers")]),e._v(". This project addresses a different part of the problem explained above: how to use statistical taggers and parsers to speed up annotation. The goal of this project is to hook a set of standard taggers up with the PROIEL XML pipeline so that we can train them easily from the command line without any additional configuration. You should think carefully about exactly which taggers you want to support, what formats they assume for their input data, and what sort of external dependencies they have. Bonus points if you put together, for example, a Docker image. Solid documentation is also required. "),t("em",[e._v("You need: Ruby, a bit of experience with taggers")])])])])])}),[],!1,null,null,null);t.default=a.exports}}]);